{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf2c38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Якщо запускати цей код на колабі, можливо прийдеться додати на диск файл з даними, та поміняти шляхи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e7e16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text) \n",
    "    text = re.sub(r'^\"+|\"+$', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r\"[ʼ’‘`´]\", \"'\", text)\n",
    "    text = re.sub(r\"[–—−]\", \"-\", text)\n",
    "    text = re.sub(r\"[«»“”]\", '\"', text)\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def capitalize_text(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    return re.sub(\n",
    "        r'(^|[.!?]\\s+)([а-яіїєґa-z])', \n",
    "        lambda m: m.group(1) + m.group(2).upper(), \n",
    "        text\n",
    "    )\n",
    "\n",
    "def mask_pii(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '<EMAIL>', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub(r'(\\+?38)?\\s?\\(?0\\d{2}\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{2}[\\s\\-]?\\d{2}', '<PHONE>', text)\n",
    "    return text\n",
    "\n",
    "def sentence_split(text: str) -> list[str]:\n",
    "    if not text: return []\n",
    "    protected_abbrs = [r'\\bм\\.', r'\\bвул\\.', r'\\bр\\.', r'\\bт\\.д\\.', r'\\bт\\.п\\.', r'\\bгрн\\.']\n",
    "    \n",
    "    for abbr in protected_abbrs:\n",
    "        text = re.sub(abbr, lambda m: m.group(0).replace('.', '<DOT>'), text)\n",
    "    text = re.sub(r'(\\d)\\.(\\d)', r'\\1<DOT>\\2', text)\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[А-ЯІЇЄҐA-Z])', text)\n",
    "    return [s.replace('<DOT>', '.') for s in sentences]\n",
    "\n",
    "def preprocess(text: str) -> dict:\n",
    "    clean = capitalize_text(mask_pii(normalize_text(clean_text(text))))\n",
    "    sentences = sentence_split(clean)\n",
    "    return {\"clean\": clean, \"sentences\": sentences}\n",
    "\n",
    "\n",
    "def setup_edge_cases(input_csv: str, output_jsonl: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    edge_cases = []\n",
    "    seen_texts = set()\n",
    "\n",
    "    rules = [\n",
    "        (r'\\s+[.,!?]', \"Видалити відірвану пунктуацію (пробіл перед знаком)\"),\n",
    "        (r'^\".*\"$', \"Видалити зовнішні подвійні лапки\"),\n",
    "        (r'[«»“”]', \"Звести типографські лапки до стандартних ASCII\"),\n",
    "        (r'\\d+\\.\\d+', \"Не розбивати речення на десятковому дробі\"),\n",
    "        (r'\\b(м\\.|вул\\.|р\\.|грн\\.|ім\\.)', \"Не розбивати речення на абревіатурах\"),\n",
    "        (r'\\s+[-–—−]\\s+', \"Нормалізувати тире між словами\"),\n",
    "        (r'\\w+[-–—−]\\w+', \"Нормалізувати дефіси всередині слів\"),\n",
    "        (r'\\b\\d+\\b', \"Коректний sentence split (не розривати на цифрах)\"),\n",
    "        (r'(^|[.!?]\\s+)[а-яіїєґa-z]', \"Капіталізувати першу літеру речення\")\n",
    "    ]\n",
    "\n",
    "    MAX_PER_RULE = 5\n",
    "    rule_counts = {expected: 0 for _, expected in rules}\n",
    "\n",
    "    texts = pd.concat([df['premise_clean'], df['hypothesis_clean']]).dropna().astype(str).tolist()\n",
    "\n",
    "    case_id = 1\n",
    "    for text in texts:\n",
    "        if len(edge_cases) >= 20: \n",
    "            break \n",
    "        if text in seen_texts:\n",
    "            continue\n",
    "\n",
    "        for pattern, expected in rules:\n",
    "            if rule_counts[expected] >= MAX_PER_RULE:\n",
    "                continue \n",
    "            \n",
    "            if re.search(pattern, text):\n",
    "                edge_cases.append({\n",
    "                    \"id\": f\"real_edge_{case_id:02d}\",\n",
    "                    \"raw_text\": text,\n",
    "                    \"expected_behavior\": expected\n",
    "                })\n",
    "                seen_texts.add(text)\n",
    "                rule_counts[expected] += 1\n",
    "                case_id += 1\n",
    "                break \n",
    "\n",
    "    os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)\n",
    "    with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "        for case in edge_cases:\n",
    "            f.write(json.dumps(case, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "def print_pipeline_stats(df):\n",
    "    cols = ['premise_clean', 'hypothesis_clean']\n",
    "    total_stats = {\n",
    "        'Відірвана пунктуація (пробіли перед крапками/комами)': 0,\n",
    "        'Зайві зовнішні лапки (CSV артефакти)': 0,\n",
    "        'Нетипові апострофи (ʼ’‘`´)': 0,\n",
    "        'Типографські тире (–—−)': 0,\n",
    "        'Множинні пробіли': 0,\n",
    "        'Речення з малої літери (буде капіталізовано)': 0\n",
    "    }\n",
    "    \n",
    "    for col in cols:\n",
    "        total_stats['Відірвана пунктуація (пробіли перед крапками/комами)'] += df[col].str.contains(r'\\s+[.,!?]', regex=True, na=False).sum()\n",
    "        total_stats['Зайві зовнішні лапки (CSV артефакти)'] += df[col].str.contains(r'^\"+|\"+$', regex=True, na=False).sum()\n",
    "        total_stats['Нетипові апострофи (ʼ’‘`´)'] += df[col].str.contains(r\"[ʼ’‘`´]\", regex=True, na=False).sum()\n",
    "        total_stats['Типографські тире (–—−)'] += df[col].str.contains(r\"[–—−]\", regex=True, na=False).sum()\n",
    "        total_stats['Множинні пробіли'] += df[col].str.contains(r'\\s{2,}', regex=True, na=False).sum()\n",
    "        total_stats['Речення з малої літери (буде капіталізовано)'] += df[col].str.contains(r'(^|[.!?]\\s+)[а-яіїєґa-z]', regex=True, na=False).sum()\n",
    "\n",
    "    print(\"\\nСтатистика\")\n",
    "    for k, v in total_stats.items():\n",
    "        print(f\"  - {k}: {v} шт.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def run_regression_tests():\n",
    "    print(\"\\n[Regression test]\")\n",
    "    sample = \"чоловік готується кидати палку за свою собаку .\" \n",
    "    step1 = preprocess(sample)['clean']\n",
    "    step2 = preprocess(step1)['clean']\n",
    "    assert step1 == \"Чоловік готується кидати палку за свою собаку.\", \"Помилка капіталізації або пунктуації!\"\n",
    "    assert step1 == step2, \"Fail: Idempotence\"\n",
    "    print(\"Success: Idempotence & Capitalization tests passed\")\n",
    "\n",
    "def main():\n",
    "    RAW_DATA_PATH = '/Users/danylo/Desktop/coursework/lab2/data/processed.csv' \n",
    "    PROCESSED_DATA_PATH = '/Users/danylo/Desktop/coursework/lab2/data/processed_v2.csv'\n",
    "    EDGE_CASES_PATH = '/Users/danylo/Desktop/coursework/lab2/tests/edge_cases.jsonl'   \n",
    "     \n",
    "    setup_edge_cases(RAW_DATA_PATH, EDGE_CASES_PATH)\n",
    "    run_regression_tests()\n",
    "    \n",
    "    print(\"\\n[Data processing]\")\n",
    "    try:\n",
    "        df = pd.read_csv(RAW_DATA_PATH)\n",
    "        print(f\"Завантажено рядків: {len(df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Файл {RAW_DATA_PATH} не знайдено\")\n",
    "        return\n",
    "\n",
    "    print_pipeline_stats(df)\n",
    "\n",
    "    df['premise_dict'] = df['premise_clean'].apply(lambda x: preprocess(str(x)))\n",
    "    df['hypothesis_dict'] = df['hypothesis_clean'].apply(lambda x: preprocess(str(x)))\n",
    "    df['premise_v2'] = df['premise_dict'].apply(lambda x: x['clean'])\n",
    "    df['hypothesis_v2'] = df['hypothesis_dict'].apply(lambda x: x['clean'])\n",
    "    df['premise_sentences'] = df['premise_dict'].apply(lambda x: x['sentences'])\n",
    "    df['hypothesis_sentences'] = df['hypothesis_dict'].apply(lambda x: x['sentences'])\n",
    "\n",
    "    empty_premises = df[df['premise_v2'] == ''].shape[0]\n",
    "    print(f\"\\nЗнайдено {empty_premises} порожніх premise.\")\n",
    "\n",
    "    empty_hypothesis = df[df['hypothesis_v2'] == ''].shape[0]\n",
    "    print(f\"\\nЗнайдено {empty_hypothesis} порожніх hypothesis.\")\n",
    "    \n",
    "    duplicates_before = df.duplicated(subset=['premise_clean', 'hypothesis_clean']).sum()\n",
    "    duplicates_after = df.duplicated(subset=['premise_v2', 'hypothesis_v2']).sum()\n",
    "    print(f\"Дублікати ДО: {duplicates_before}\")\n",
    "    print(f\"Дублікати ПІСЛЯ нормалізації: {duplicates_after}\")\n",
    "\n",
    "    print(\"\\n5 Прикладів До/Після \")\n",
    "    \n",
    "    mask_quotes = df['premise_clean'].str.contains(r'^\"+|\"+$', regex=True, na=False) | \\\n",
    "                  df['hypothesis_clean'].str.contains(r'^\"+|\"+$', regex=True, na=False)\n",
    "    df_quotes = df[mask_quotes].head(2)\n",
    "    \n",
    "    mask_others = ((df['premise_clean'] != df['premise_v2']) | \\\n",
    "                   (df['hypothesis_clean'] != df['hypothesis_v2'])) & ~mask_quotes\n",
    "    df_others = df[mask_others].head(4)\n",
    "    \n",
    "    demo_df = pd.concat([df_quotes, df_others]).head(5)\n",
    "    if demo_df.empty: demo_df = df.head(5)\n",
    "    \n",
    "    for _, row in demo_df.iterrows():\n",
    "        if re.search(r'^\"+|\"+$', str(row['premise_clean'])) or (row['premise_clean'] != row['premise_v2']):\n",
    "            print(f\"RAW:   {row['premise_clean']}\")\n",
    "            print(f\"CLEAN: {row['premise_v2']}\")\n",
    "        elif row['hypothesis_clean'] != row['hypothesis_v2']:\n",
    "            print(f\"RAW:   {row['hypothesis_clean']}\")\n",
    "            print(f\"CLEAN: {row['hypothesis_v2']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    os.makedirs(os.path.dirname(PROCESSED_DATA_PATH), exist_ok=True)\n",
    "    output_cols = ['premise_v2', 'hypothesis_v2', 'premise_sentences', 'hypothesis_sentences', 'labels', 'label_name']\n",
    "    df[output_cols].to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "    print(f\"\\nДані збережено у {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
